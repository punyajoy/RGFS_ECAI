{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T05:43:35.997914Z",
     "start_time": "2021-11-13T05:43:31.824188Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/nlp/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/nlp/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from glob import glob\n",
    "from Data_code.utils import returnMask_test\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "import more_itertools as mit\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T05:43:38.521016Z",
     "start_time": "2021-11-13T05:43:35.999148Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=False)\n",
    "def get_training_data(dict_map):\n",
    "    final_output = []\n",
    "    for key in dict_map.keys():\n",
    "        print(dict_map[key])\n",
    "        annotation=dict_map[key]['label']\n",
    "        text=dict_map[key]['text']\n",
    "        post_id=key\n",
    "        tokens_all,attention_masks=returnMask_test(dict_map[key], tokenizer)\n",
    "        final_output.append([post_id, annotation, tokens_all, attention_masks])\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T05:43:38.542674Z",
     "start_time": "2021-11-13T05:43:38.522334Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
    "def find_ranges(iterable):\n",
    "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
    "    for group in mit.consecutive_groups(iterable):\n",
    "        group = list(group)\n",
    "        if len(group) == 1:\n",
    "            yield group[0]\n",
    "        else:\n",
    "            yield group[0], group[-1]\n",
    "            \n",
    "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
    "def get_evidence(post_id, anno_text, explanations):\n",
    "    output = []\n",
    "\n",
    "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
    "    span_list = list(find_ranges(indexes))\n",
    "\n",
    "    for each in span_list:\n",
    "        if type(each)== int:\n",
    "            start = each\n",
    "            end = each+1\n",
    "        elif len(each) == 2:\n",
    "            start = each[0]\n",
    "            end = each[1]+1\n",
    "        else:\n",
    "            print('error')\n",
    "\n",
    "        output.append({\"docid\":post_id, \n",
    "              \"end_sentence\": -1, \n",
    "              \"end_token\": end, \n",
    "              \"start_sentence\": -1, \n",
    "              \"start_token\": start, \n",
    "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
    "    return output\n",
    "\n",
    "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
    "def convert_to_eraser_format(dataset, method, save_path, save_split):  \n",
    "    final_output = []\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    if save_split:\n",
    "#         train_fp = open(save_path+'train.jsonl', 'w')\n",
    "#         val_fp = open(save_path+'val.jsonl', 'w')\n",
    "        test_fp = open(save_path+'test.jsonl', 'w')\n",
    "            \n",
    "    for tcount, eachrow in enumerate(dataset):\n",
    "        \n",
    "        temp = {}\n",
    "        post_id = eachrow[0]\n",
    "        post_class = eachrow[1]\n",
    "        anno_text_list = eachrow[2]\n",
    "        majority_label = eachrow[1]\n",
    "        \n",
    "        if majority_label=='normal':\n",
    "            continue\n",
    "        \n",
    "        print(eachrow[2])\n",
    "        #all_labels = eachrow[4]\n",
    "        explanations = []\n",
    "        for each_explain in eachrow[3]:\n",
    "            print(each_explain)\n",
    "            explanations.append(list(each_explain))\n",
    "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
    "        if method == 'union':\n",
    "            final_explanation = [any(each) for each in zip(*explanations)]\n",
    "            final_explanation = [int(each) for each in final_explanation]\n",
    "        \n",
    "        print(len(final_explanation))\n",
    "        print(len(eachrow[2]))\n",
    "        \n",
    "        assert(len(final_explanation)==len(eachrow[2]))\n",
    "        print(\"==========================\")\n",
    "        \n",
    "        temp['annotation_id'] = post_id\n",
    "        temp['classification'] = post_class\n",
    "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
    "        temp['query'] = \"What is the class?\"\n",
    "        temp['query_type'] = None\n",
    "        final_output.append(temp)\n",
    "        \n",
    "        if save_split:\n",
    "            if not os.path.exists(save_path+'docs'):\n",
    "                os.makedirs(save_path+'docs')\n",
    "            \n",
    "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
    "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
    "            test_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "#             if post_id in id_division['train']:\n",
    "#                 train_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "#             elif post_id in id_division['val']:\n",
    "#                 val_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "#             elif post_id in id_division['test']:\n",
    "#                 test_fp.write(json.dumps(temp)+'\\n')\n",
    "#             else:\n",
    "#                 print(post_id)\n",
    "    \n",
    "    if save_split:\n",
    "        test_fp.close()\n",
    "        \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T15:19:32.714122Z",
     "start_time": "2021-11-14T15:19:32.624132Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" @user @user maybe I'm over keen with the ridiculous feminazi brigade. The tweets about rape culture were directly out of sarah!\", 'annotation': [[[53, 61]], [[53, 69]]], 'label': 'sexism'}\n",
      "{'text': ' #BlameOneNotAll because Benedict Cumberbatch, David Tennant, Jared Padalecki, and Jensen Ackles must all be rapists too, right tumblr girls?', 'annotation': [[[102, 120], [128, 140]], [[17, 140]]], 'label': 'sexism'}\n",
      "{'text': ' @user @user Women must be addressed as milady until they turn you down at which point the correct form of address is \"feminazi\"', 'annotation': [[[119, 127]], [[72, 128]]], 'label': 'sexism'}\n",
      "{'text': ' @user @user i ship you 2 angry feminazi\\'s together ur hate would b so cute together \"i hate this thing\" \"so do i\" \"omg yas\"', 'annotation': [[[32, 43]], [[15, 42]]], 'label': 'sexism'}\n",
      "{'text': ' \"Fukken anita sarkesian doesnt allow responses to her videos!! Feminazi!!\"  http://url  *gets blocked by roguestar, AGAIN*', 'annotation': [[[64, 72], [106, 115], [2, 8]], [[64, 72], [2, 14]]], 'label': 'sexism'}\n",
      "{'text': ' Everytime I see someone share a Feminazi post I get one step closer to having a heart attack ğŸŒ‹ You need to CHECK YOURSELF', 'annotation': [[[33, 41]], [[108, 122]]], 'label': 'sexism'}\n",
      "{'text': ' baby you can drive my car http://url #asian #asianbabes #asiangirls #xxx #nsfw', 'annotation': [[[69, 73], [45, 56]], [[46, 56], [70, 73]]], 'label': 'sexism'}\n",
      "{'text': ' @user feminazi', 'annotation': [[[7, 15]], [[7, 15]]], 'label': 'sexism'}\n",
      "{'text': \" I support #GamerGate because I'm a Gamer and i don't want a bunch of corrupt Media Feminazi's ruining video games for everyone.\", 'annotation': [[[84, 95]], [[70, 114]]], 'label': 'sexism'}\n",
      "{'text': \" 'To get rid of Jeremy is part and parcel of the feminazi thought-police tactics that are strangling this country.' http://url\", 'annotation': [[[49, 57]], [[48, 80], [90, 113]]], 'label': 'sexism'}\n",
      "{'text': \" @user she's a feminazi tumblr shit, ah were dealing witj a real adult here\", 'annotation': [[[15, 30]], [[53, 70], [15, 35]]], 'label': 'sexism'}\n",
      "{'text': ' @user feminazi bullshit', 'annotation': [[[16, 24], [7, 16]], [[7, 24]]], 'label': 'sexism'}\n",
      "{'text': \" Katie and Nikki have definitely proved they're not just dumb blondes with pretty faces. They're just dumb blondes. #mkr\", 'annotation': [[[102, 115]], [[89, 115]]], 'label': 'sexism'}\n",
      "{'text': \" RT @user: Feminazi Blog Reminds Liberals Today Is 'National Day of Appreciation for Abortion Providers'â€¦ http://url\", 'annotation': [[[11, 24]], [[52, 103]]], 'label': 'sexism'}\n",
      "{'text': \" ''feminazi''\", 'annotation': [[[1, 13]], [[1, 13]]], 'label': 'sexism'}\n",
      "{'text': ' RT @user: @user that is just unacceptable. Classic Feminazi Lesbian Liberalism stifling freedom of speech. Or something @userâ€¦', 'annotation': [[[52, 61]], [[51, 68]]], 'label': 'sexism'}\n",
      "{'text': ' @user  And where are the #feminazi brigades on this topic?? &lt;&lt;crickets&gt;&gt;', 'annotation': [[[26, 44]], [[26, 44]]], 'label': 'sexism'}\n",
      "{'text': \" I did it. I made a feminist admit she's been beat! #feminazi #gottold\", 'annotation': [[[52, 61]], [[52, 62]]], 'label': 'sexism'}\n",
      "{'text': ' RT @user: http://url @user @user  #IfWomenRuledTheWorld #FeminismIsAwful #feminazi #WomenAgainstFeminism', 'annotation': [[[74, 83]], [[57, 74]]], 'label': 'sexism'}\n",
      "{'text': ' There goes the man-woman wage-gap sham in shambles!..... like all other Feminazi falsities! http://url', 'annotation': [[[73, 81]], [[72, 91]]], 'label': 'sexism'}\n",
      "{'text': \" Take a feminazi to church and you'd start a riot I swear\", 'annotation': [[[37, 49], [8, 16]], [[1, 49]]], 'label': 'sexism'}\n",
      "{'text': ' Gee, the useless feminazi #BaharMustafa gets to keep the job she no doubt fails to do, all while still preaching her Manson-level cult shit.', 'annotation': [[[10, 26]], [[10, 141]]], 'label': 'sexism'}\n",
      "{'text': \" And that's why they invented the term feminazi... http://url\", 'annotation': [[[39, 47]], [[5, 47]]], 'label': 'sexism'}\n",
      "{'text': ' \"Women are psychos\"  *male tears joke gets made*  \"SHUT THE FUCK UP DUMB FEMINAZI BITCH!! THIS IS WHY I NEED MENINISM!!\"', 'annotation': [[[110, 118], [61, 88], [2, 19]], [[91, 118], [52, 88]]], 'label': 'sexism'}\n",
      "{'text': \" @user @user Not fascism, no no no. Just 'common sense nationalism'. What are you: some kind of leftard feminazi?!\", 'annotation': [[[96, 112]], [[88, 112]]], 'label': 'sexism'}\n",
      "{'text': ' I want a feminazi to argue with', 'annotation': [[[10, 19]], [[3, 18]]], 'label': 'sexism'}\n",
      "{'text': ' @user feminazi :v', 'annotation': [[[7, 16]], [[7, 18]]], 'label': 'sexism'}\n",
      "{'text': ' How the f#*k are they \"models\"? #MKR', 'annotation': [[[9, 31]], [[9, 13]]], 'label': 'sexism'}\n",
      "{'text': ' @user feminazi', 'annotation': [[[7, 15]], [[7, 15]]], 'label': 'sexism'}\n",
      "{'text': \" Vicar, then Feminazi Granger, then OH, WHAT'S OCCURIN?! What a cast xD\", 'annotation': [[[13, 21]], [[13, 29]]], 'label': 'sexism'}\n",
      "{'text': ' .@user @user she is a feminazi, also I am proud to have cartoon plastic surgery', 'annotation': [[[23, 32]], [[23, 31]]], 'label': 'sexism'}\n",
      "{'text': ' baby you can drive my car http://url #asian #juicyasian #sex #nsfw #adult #xxx', 'annotation': [[[46, 57]], [[1, 27], [63, 67], [58, 61], [76, 79]]], 'label': 'sexism'}\n",
      "{'text': \" RT @user: @user @user @user @user we'd be ranting feminazi bitches during the wrong time of the month http://url\", 'annotation': [[[51, 67]], [[51, 67]]], 'label': 'sexism'}\n",
      "{'text': \" @user I'll take that as a compliment coming from a feminazi.\", 'annotation': [[[52, 60]], [[7, 61]]], 'label': 'sexism'}\n",
      "{'text': ' @user @user feminazi spotted', 'annotation': [[[13, 21]], [[13, 21]]], 'label': 'sexism'}\n",
      "{'text': \" I'm tired of all you feminazi's\", 'annotation': [[[22, 32]], [[1, 32]]], 'label': 'sexism'}\n",
      "{'text': ' @user @user @user Yeah, who the fuck is Dr Brooke Magnanti?! Probably some kind of man-hating feminazi.', 'annotation': [[[33, 37], [95, 104]], [[84, 104]]], 'label': 'sexism'}\n",
      "{'text': ' @user @user @user @user I pictured her as a man can have a man wants rights, feminazi swarm like piranhas', 'annotation': [[[78, 86]], [[78, 106]]], 'label': 'sexism'}\n",
      "{'text': ' @user quit cryin ya fuckin coon the dumshit shoukdnt fucked with tha cops yard ape got had @user @user @user #coon', 'annotation': [[[54, 74]], [[54, 74], [37, 44], [21, 32]]], 'label': 'sexism'}\n",
      "{'text': \" Feminazi's\", 'annotation': [[[1, 9]], [[1, 11]]], 'label': 'sexism'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "datasets=['Davidson','Waseem']\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_path='Dataset/Eraser_eval/'\n",
    "    save_path=dataset_path+dataset+'/'\n",
    "    method = 'union'\n",
    "    save_split=True\n",
    "    dataset_path='Dataset/Test_data/'+dataset+'_test_new.json'\n",
    "    with open(dataset_path,'r') as infile:\n",
    "        dict_map=json.load(infile)\n",
    "    final_output=get_training_data(dict_map)\n",
    "    #convert_to_eraser_format(final_output, method, save_path,save_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:56:45.102587Z",
     "start_time": "2021-11-16T14:56:45.086268Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "test_fp = open('../eraserbenchmark/bash_eraser_test_new.sh', 'w')\n",
    "#\n",
    "for dataset in ['Founta','Basile','Davidson','Olid','Waseem']:\n",
    "    eval_files=glob.glob('../Hate_domain_adaptation/explanation_dicts/'+dataset+'*')\n",
    "    for file in eval_files:\n",
    "        str1='PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test --strict --data_dir ../Hate_domain_adaptation/Dataset/Eraser_eval/'+dataset+'/ --results '+file+' --score_file '+ '../Hate_domain_adaptation/eraser_explanation_dicts/'+file.split('/')[3]\n",
    "        test_fp.write(str1+'\\n')\n",
    "#     eval_files=glob.glob('../Hate_domain_adaptation/explanation_dicts/'+dataset+'_rationale_BERT_toxic_rationale_10_keyword*')\n",
    "#     for file in eval_files:\n",
    "#         str1='PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test --strict --data_dir ../Hate_domain_adaptation/Dataset/Eraser_eval/'+dataset+'/ --results '+file+' --score_file '+ '../Hate_domain_adaptation/eraser_explanation_dicts/'+file.split('/')[3]\n",
    "#         test_fp.write(str1+'\\n')\n",
    "test_fp.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T09:11:54.577390Z",
     "start_time": "2022-04-26T09:11:54.572712Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "test_fp = open('../eraserbenchmark/bash_eraser_test_shap.sh', 'w')\n",
    "#\n",
    "for dataset in ['Basile','Davidson','Founta','Waseem','Mlma','Olid']:\n",
    "    eval_files=glob.glob('../Hate_domain_adaptation/explanation_dicts_shap/'+dataset+'*')\n",
    "    for file in eval_files:\n",
    "        str1='PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test --strict --data_dir ../Hate_domain_adaptation/Dataset/Eraser_eval/'+dataset+'/ --results '+file+' --score_file '+ '../Hate_domain_adaptation/eraser_explanation_dicts_shap/'+file.split('/')[3]\n",
    "        test_fp.write(str1+'\\n')\n",
    "#     eval_files=glob.glob('../Hate_domain_adaptation/explanation_dicts/'+dataset+'_rationale_BERT_toxic_rationale_10_keyword*')\n",
    "#     for file in eval_files:\n",
    "#         str1='PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test --strict --data_dir ../Hate_domain_adaptation/Dataset/Eraser_eval/'+dataset+'/ --results '+file+' --score_file '+ '../Hate_domain_adaptation/eraser_explanation_dicts/'+file.split('/')[3]\n",
    "#         test_fp.write(str1+'\\n')\n",
    "test_fp.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T16:31:45.958408Z",
     "start_time": "2022-04-26T16:31:45.956459Z"
    }
   },
   "outputs": [],
   "source": [
    "files_eraser_eval=glob.glob('../Hate_domain_adaptation/eraser_explanation_dicts_shap/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T16:31:46.280901Z",
     "start_time": "2022-04-26T16:31:46.264488Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_files_group={}\n",
    "\n",
    "for file  in files_eraser_eval:\n",
    "    \n",
    "    if('rationale' in file):\n",
    "        try:\n",
    "            dict_files_group[file[:-23]+'_rationale'].append(file)\n",
    "        except:\n",
    "            dict_files_group[file[:-23]+'_rationale']=[file]\n",
    "    if('shap' in file):\n",
    "        try:\n",
    "            dict_files_group[file[:-18]].append(file)\n",
    "        except:\n",
    "            dict_files_group[file[:-18]]=[file]\n",
    "    else:\n",
    "        try:\n",
    "            dict_files_group[file[:-13]].append(file)\n",
    "        except:\n",
    "            dict_files_group[file[:-13]]=[file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T16:31:46.700476Z",
     "start_time": "2022-04-26T16:31:46.690293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2021_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_cd': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_cd_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_cd_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_cd_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_cd_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_cd_2024_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_cd': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_cd_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_cd_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_cd_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_cd_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_cd_2025_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_2024_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_2023_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_cd': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_cd_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_cd_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_cd_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_cd_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Basile_bert_cd_2021_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_2025_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_cd': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_cd_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_cd_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_cd_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_cd_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Founta_bert_cd_2023_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_2022_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Davidson_bert_2025_50_shap.json'],\n",
       " '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_cd': ['../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_cd_2021_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_cd_2023_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_cd_2024_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_cd_2025_50_shap.json',\n",
       "  '../Hate_domain_adaptation/eraser_explanation_dicts_shap/Olid_bert_cd_2022_50_shap.json']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_files_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T16:31:47.205497Z",
     "start_time": "2022-04-26T16:31:47.202348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2021_50_shap.json\n",
      "../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2023_50_shap.json\n",
      "../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2024_50_shap.json\n",
      "../Hate_domain_adaptation/eraser_explanation_dicts_shap/Waseem_bert_2025_50_shap.json\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "for key in dict_files_group:\n",
    "    #print(key,len(dict_files_group[key]))\n",
    "    if(len(dict_files_group[key])<5):\n",
    "        for ele in sorted(dict_files_group[key]):\n",
    "            print(ele)\n",
    "        print(\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T17:28:13.459393Z",
     "start_time": "2022-04-26T17:28:13.454394Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_metric_dict(name_of_metric):\n",
    "    result={}\n",
    "    for key in dict_files_group:\n",
    "        \n",
    "#         if('bert_cd' in key):\n",
    "#             continue\n",
    "#         if('rationale' in key):\n",
    "#             pass\n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "        value=0\n",
    "        for file in dict_files_group[key]:\n",
    "            \n",
    "            \n",
    "            \n",
    "            with open(file) as fp:\n",
    "                output_data = json.load(fp)\n",
    "            if(name_of_metric=='auprc'):\n",
    "                value+=output_data['token_soft_metrics']['auprc']\n",
    "            elif(name_of_metric=='token_f1'):\n",
    "                value+=output_data['token_prf']['instance_macro']['f1']\n",
    "            elif(name_of_metric=='iou_f1'):\n",
    "                value+=output_data['iou_scores'][0]['macro']['f1']\n",
    "            elif(name_of_metric=='sufficiency'):\n",
    "                value+=output_data['classification_scores']['sufficiency']\n",
    "            elif(name_of_metric=='comprehensiveness'):\n",
    "                value+=output_data['classification_scores']['comprehensiveness']\n",
    "        result[key[56:]]=value/len(dict_files_group[key])\n",
    "    result={k: v for k, v in sorted(result.items(), key=lambda item: item[0])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T17:28:13.905664Z",
     "start_time": "2022-04-26T17:28:13.894095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davidson_bert 0.3718791250609757\n",
      "Davidson_bert_cd 0.45887803832244944\n",
      "Olid_bert 0.33859450530026797\n",
      "Olid_bert_cd 0.37038165787772687\n",
      "Basile_bert 0.3499836184241008\n",
      "Basile_bert_cd 0.4646319531471774\n",
      "Founta_bert 0.37717342043472024\n",
      "Founta_bert_cd 0.48022494289719486\n",
      "Waseem_bert 0.4588233935581439\n",
      "Waseem_bert_cd 0.5060585586909346\n"
     ]
    }
   ],
   "source": [
    "name_of_metric='auprc'\n",
    "results=get_metric_dict(name_of_metric)\n",
    "\n",
    "for dataset in ['Davidson','Olid','Basile','Founta','Waseem','MLMA']:\n",
    "    for model in ['bert','bert_cd','Transform_Rationale_CrossAttn_CLS_Drpt_corrected_rationale','Transform_Rationale_SelfAttn_Drpt_corrected_rationale']:\n",
    "        try:\n",
    "            print(dataset+'_'+model,results[dataset+'_'+model])\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T17:28:19.242442Z",
     "start_time": "2022-04-26T17:28:19.236591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davidson_bert 0.27879043734043735\n",
      "Davidson_bert_cd 0.37136456876456875\n",
      "Olid_bert 0.2815060570173352\n",
      "Olid_bert_cd 0.3225729859113318\n",
      "Basile_bert 0.2522912826960009\n",
      "Basile_bert_cd 0.3482836427137158\n",
      "Founta_bert 0.254518860608826\n",
      "Founta_bert_cd 0.3598764077137779\n",
      "Waseem_bert 0.3175349302937345\n",
      "Waseem_bert_cd 0.34941564382469953\n"
     ]
    }
   ],
   "source": [
    "name_of_metric='token_f1'\n",
    "results=get_metric_dict(name_of_metric)\n",
    "\n",
    "for dataset in ['Davidson','Olid','Basile','Founta','Waseem','MLMA']:\n",
    "    for model in ['bert','bert_cd','Transform_Rationale_CrossAttn_CLS_Drpt_corrected_rationale','Transform_Rationale_SelfAttn_Drpt_corrected_rationale']:\n",
    "        try:\n",
    "            print(dataset+'_'+model,results[dataset+'_'+model])\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T09:21:41.752583Z",
     "start_time": "2022-04-26T09:21:41.745998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davidson_bert 0.08515643416770688\n",
      "Davidson_bert_cd 0.14255991607568586\n",
      "Olid_bert 0.06185808817664673\n",
      "Olid_bert_cd 0.07676725473897257\n",
      "Basile_bert 0.10062890108371617\n",
      "Basile_bert_cd 0.1449496033789166\n",
      "Founta_bert 0.05719341949586768\n",
      "Founta_bert_cd 0.06129325358063292\n",
      "Waseem_bert 0.0078125\n",
      "Waseem_bert_cd 0.0075\n"
     ]
    }
   ],
   "source": [
    "name_of_metric='iou_f1'\n",
    "results=get_metric_dict(name_of_metric)\n",
    "\n",
    "for dataset in ['Davidson','Olid','Basile','Founta','Waseem']:\n",
    "    for model in ['bert','bert_cd']:\n",
    "        print(dataset+'_'+model,results[dataset+'_'+model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T09:22:08.482711Z",
     "start_time": "2022-04-26T09:22:08.475630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davidson_bert 0.028385287392884494\n",
      "Davidson_bert_cd -0.2956497736182064\n",
      "Olid_bert -0.17194013442311967\n",
      "Olid_bert_cd -0.10685329990727561\n",
      "Basile_bert -0.14387096044421196\n",
      "Basile_bert_cd -0.3864719812870026\n",
      "Founta_bert -0.03506338210666881\n",
      "Founta_bert_cd -0.1401792105506448\n",
      "Waseem_bert 0.08628971884027124\n",
      "Waseem_bert_cd -0.010902608055621384\n"
     ]
    }
   ],
   "source": [
    "name_of_metric='sufficiency'\n",
    "results=get_metric_dict(name_of_metric)\n",
    "\n",
    "for dataset in ['Davidson','Olid','Basile','Founta','Waseem']:\n",
    "    for model in ['bert','bert_cd']:\n",
    "        print(dataset+'_'+model,results[dataset+'_'+model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T09:22:21.225701Z",
     "start_time": "2022-04-26T09:22:21.220391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davidson_bert 0.03512217603623866\n",
      "Davidson_bert_cd -0.14006396474316718\n",
      "Olid_bert -0.14291966323341643\n",
      "Olid_bert_cd -0.09816070760999408\n",
      "Basile_bert -0.20817292173206808\n",
      "Basile_bert_cd -0.3802526766061783\n",
      "Founta_bert 0.008470927222686659\n",
      "Founta_bert_cd -0.09938222559935908\n",
      "Waseem_bert -0.014074154570698744\n",
      "Waseem_bert_cd -0.09447050668299198\n"
     ]
    }
   ],
   "source": [
    "name_of_metric='comprehensiveness'\n",
    "results=get_metric_dict(name_of_metric)\n",
    "\n",
    "for dataset in ['Davidson','Olid','Basile','Founta','Waseem']:\n",
    "    for model in ['bert','bert_cd']:\n",
    "        print(dataset+'_'+model,results[dataset+'_'+model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
